---
title: "Final_peer(2)"
author: "jchong911"
date: "12/24/2020"
output: html_document
---

```{r packages, message = FALSE}
library(statsr)
library(dplyr)
library(BAS)
library(MASS)
library(corrplot)
library(ggplot2)
library(knitr)
library(GGally)
library(gridExtra)

load("ames_train.Rdata")

```

### Section 2.3 Initial Model Residuals
One way to assess the performance of a model is to examine the model's residuals. In the space below, create a residual plot for your preferred model from above and use it to assess whether your model appears to fit the data well. Comment on any interesting structure in the residual plot (trend, outliers, etc.) and briefly discuss potential implications it may have for your model and inference / prediction you might produce.

* * *
#### Residuals Plot 

```{r model_resid}
model.bas <- bas.lm(log(price) ~ Overall.Qual + Garage.Area +
                   Total.Bsmt.SF + Garage.Cars + log(area) +
                   Full.Bath + Bedroom.AbvGr + Year.Built +
                   X1st.Flr.SF + Lot.Area, data = ames_train,
                   prior = "AIC", modelprior=uniform())
par(mfrow = c(2,2))
plot(model.bas)
```

The Residuals vs Fitted plot is used to check the linear relationship assumptions. A horizontal line, without distinct patterns is an indication for a linear relationship, what is good. In our example, there is no pattern in the residual plot. This suggests that we can assume linear relationship between the predictors and the outcome variables. However, the model overpredicted certain houses such as house #428,181 and 310. We can find this information as follows

``` {r}
pred_train <- predict(model.bas,ames_train,estimator = "BMA")
resid_train <- na.omit(ames_train$price - exp(pred_train$fit))
data.fit.resid <- data.frame(fitted = na.omit(exp(pred_train$fit)), resid = resid_train)
overprice <- ames_train %>% dplyr::select(Lot.Area, Land.Slope, Year.Built, Year.Remod.Add, Bedroom.AbvGr, price)
overprice$predicted <- exp(pred_train$fit)
overprice[c(428,181,310),]
```

``` {r, echo = FALSE}
rm(overprice)
```

#### Q-Q plot

The QQ plot of residuals can be used to visually check the normality assumption. The normal probability plot of residuals should approximately follow a straight line.

``` {r}
mu_resid <- mean(resid_train, na.rm=TRUE)
sd_resid <- sd(resid_train, na.rm=TRUE)
std_resid <- (resid_train-mu_resid)/sd_resid
par(mfrow=c(1,2))
qqnorm(std_resid, lty = 2)
qqline(std_resid)
plot(density(std_resid), main="Probability Density of Std. Residuals", xlab="Std. Residuals", ylab="P(Std. Residuals)")
```

#### Scale Plot

``` {r}
sqrt_std_resid <- sqrt(abs(std_resid))
plot_dat <- data.frame(fitted = na.omit(exp(pred_train$fit)), resid = resid_train, sqrt_std_resid = sqrt_std_resid)
ggplot(plot_dat, aes(x = fitted, y = sqrt_std_resid)) +
  geom_point(colour = 'darkgreen') + 
  geom_smooth(method = 'loess', color= "blue", lwd = 0.5) + 
  labs(title = "Scale-Location Plot", y = "Sqrt(Std. Residuals)", x = "Fitted values") 
```

* * *

### Section 2.4 Initial Model RMSE

You can calculate it directly based on the model output. Be specific about the units of your RMSE (depending on whether you transformed your response variable). The value you report will be more meaningful if it is in the original units (dollars).

* * *

``` {r, echo = FALSE, message = FALSE}
gc()
```

```{r model_rmse}
model.bas <- bas.lm(log(price) ~ Overall.Qual + Garage.Area +
                   Total.Bsmt.SF + Garage.Cars + log(area) +
                   Full.Bath + Bedroom.AbvGr + Year.Built +
                   X1st.Flr.SF + Lot.Area, data = ames_train,
                   prior = "AIC", modelprior=uniform())

pred_train <- predict(model.bas,ames_train,estimator = "BMA")
resid_train <- na.omit(ames_train$price - exp(pred_train$fit))
rmse_train <- sqrt(mean(resid_train^2))
paste('RMSE for BMA model under ames_train = ', format(rmse_train, digit = 7))
```

* * *

### Section 2.5 Overfitting 

The process of building a model generally involves starting with an initial model (as you have done above), identifying its shortcomings, and adapting the model accordingly. This process may be repeated several times until the model fits the data reasonably well. However, the model may do well on training data but perform poorly out-of-sample (meaning, on a dataset other than the original training data) because the model is overly-tuned to specifically fit the training data. This is called “overfitting.” To determine whether overfitting is occurring on a model, compare the performance of a model on both in-sample and out-of-sample data sets. To look at performance of your initial model on out-of-sample data, you will use the data set `ames_test`.

```{r loadtest, message = FALSE}
load("ames_test.Rdata")
```

Use your model from above to generate predictions for the housing prices in the test data set.  Are the predictions significantly more accurate (compared to the actual sales prices) for the training data than the test data?  Why or why not? Briefly explain how you determined that (what steps or processes did you use)?

* * *

```{r initmodel_test}
pred_test <- predict(model.bas,ames_test,estimator = "BMA")
resid_test <- na.omit(ames_test$price - exp(pred_test$fit))
rmse_test <- sqrt(mean(resid_test^2))
paste('RMSE for BMA model under ames_test = ', format(rmse_test, digit = 7))
```

* * *

**Note to the learner:** If in real-life practice this out-of-sample analysis shows evidence that the training data fits your model a lot better than the test data, it is probably a good idea to go back and revise the model (usually by simplifying the model) to reduce this overfitting. For simplicity, we do not ask you to do this on the assignment, however.

## Part 3 Development of a Final Model

Now that you have developed an initial model to use as a baseline, create a final model with *at most* 20 variables to predict housing prices in Ames, IA, selecting from the full array of variables in the dataset and using any of the tools that we introduced in this specialization.  

Carefully document the process that you used to come up with your final model, so that you can answer the questions below.

### Section 3.1 Final Model

Provide the summary table for your model.

* * *

```{r model_playground,fig.height = 10}
ames_train = ames_train[-310,]
model.final <- bas.lm(log(price) ~ Overall.Qual + Garage.Area +
                   Total.Bsmt.SF + Garage.Cars + log(area) +
                   Full.Bath + Bedroom.AbvGr + Year.Built +
                   X1st.Flr.SF + Lot.Area + Kitchen.Qual+ Neighborhood, data = ames_train,
                   prior = "AIC", modelprior=uniform())
image(model.final, rotate = FALSE)
```

* * *

### Section 3.2 Transformation

Did you decide to transform any variables?  Why or why not? Explain in a few sentences.

In this model, we transform price and area because log-tranform both price and area provide the most linear relationship between price and area.

``` {r fig.width = 10, fig.height = 5}
# No log transform
p51 <- ggplot(ames_train, aes(x = area, y = price)) +
  geom_point() +
  stat_smooth(method = 'lm')

#Log area transform
p52 <- ggplot(ames_train, aes(x = log(area), y = price)) +
  geom_point() +
  stat_smooth(method = 'lm')

#Log price transform
p53 <- ggplot(ames_train, aes(x = area, y = log(price))) +
  geom_point() +
  stat_smooth(method = 'lm')

# Log transform both
p54 <- ggplot(ames_train, aes(x = log(area), y = log(price))) +
  geom_point() +
  stat_smooth(method = 'lm')

grid.arrange(p51, p52, p53, p54, ncol = 2)
```

* * *

### Section 3.3 Variable Interaction

Did you decide to include any variable interactions? Why or why not? Explain in a few sentences.

* * *

Use the vif function in the R package car to test for multicollinearity. - VIF = 1 : not correlated - 1< VIF < 5: moderatedly correlated - 5< VIF < 10: highly correlated

``` {r, message = FALSE}
library(car)
```

```{r model_inter}
Final.Model <- as.formula(price ~ Overall.Qual + Garage.Area +
                   Total.Bsmt.SF + Garage.Cars + area +
                   Full.Bath + Bedroom.AbvGr + Year.Built +
                   X1st.Flr.SF + Lot.Area + Kitchen.Qual)
vif(lm(Final.Model, ames_train))
```

We see that all features in our model are moderatedly or highly correlated so we will not include variable interation

* * *

### Section 3.4 Variable Selection

What method did you use to select the variables you included? Why did you select the method you used? Explain in a few sentences.

* * *

We peformed corrplot to check the correlation between predictors and price. Then we have top 14 variables that have strong relationship with price. Also, we will put some assumption about reality factor that may affect the price of a house. When we have all variables we want to analyze, we will perform BIC, AIC and BMA model to see the differences. Then we pick BMA model to perform analysis.

* * *

### Section 3.5 Model Testing

How did testing the model on out-of-sample data affect whether or how you changed your model? Explain in a few sentences.

* * *

In general, RMSE from training data will be lower than the testing data. Yet, in this analysis, when seeing RMSE from testing data lower than training data, we pick different variables to build another model and test and in all cases the testing RMSE was lower than the training RMSE and we still see that RMSE under testing data is still lower than under training data. Thus, we can conclude that there is no overfitting.

* * *

## Part 4 Final Model Assessment

### Section 4.1 Final Model Residual

For your final model, create and briefly interpret an informative plot of the residuals.

* * *

``` {r}
par(mfrow = c(2,2))
plot(model.final)
```

For Residuals vs Fitted plot, we do not see house #310 and there is no pattern in the residual plot. This suggests that we can assume linear relationship between the predictors and the outcome variables. There are houses #424, 736, 181 that the model overpredicted but they are less than 3sd so it’s acceptable.

``` {r}
pred_train2 <- predict(model.final,ames_train,estimator = "BMA")
resid_train2 <- na.omit(ames_train$price - exp(pred_train2$fit))
mu_resid2 <- mean(resid_train2, na.rm=TRUE)
sd_resid2 <- sd(resid_train2, na.rm=TRUE)
std_resid2 <- (resid_train2-mu_resid2)/sd_resid2
# Quantile-Quantile Plot of Residuals
par(mfrow=c(1,2))
qqnorm(std_resid2, lty = 2)
qqline(std_resid2)
plot(density(std_resid2), main="Probability Density of Std. Residuals", 
     xlab="Std. Residuals", ylab="P(Std. Residuals)")
```

The residuals are normally distributed out to at least two standard deviations.

``` {r}
#Scale plot
sqrt_std_resid2 <- sqrt(abs(std_resid2))
plot_dat2 <- data.frame(fitted2 = na.omit(exp(pred_train2$fit)), resid2 = resid_train2, sqrt_std_resid2 = sqrt_std_resid2)
ggplot(plot_dat2, aes(x = fitted2, y = sqrt_std_resid2)) +
  geom_point(color = 'darkgreen') + 
  geom_smooth(method = 'loess', color= "blue", lwd = 0.5) + 
  labs(title = "Scale-Location Plot for Adj. Model", y = "Sqrt(Std. Residuals)", x = "Fitted values") 
```

The variability (variances) of the residual points increases with the value of the fitted outcome variable, suggesting non-constant variances in the residuals errors (or heteroscedasticity).

* * *

### Section 4.2 Final Model RMSE

For your final model, calculate and briefly comment on the RMSE.

``` {r}
sqrt(mean(resid_train2^2))
```

``` {r, message = FALSE}
ames_test = ames_test %>% filter(Neighborhood != "Landmrk")
pred_test2 <- predict(model.final, newdata = ames_test, estimator="HPM")
```

``` {r}
test2_rmse <- sqrt(mean((exp(pred_test2$fit) - ames_test$price)^2))
test2_rmse
```

RMSE from testing data is still lower than that of training data but the difference is not as much as the old model is.

* * *

NOTE: Write your written response to section 4.2 here. Delete this note before you submit your work.

* * *

### Section 4.3 Final Model Evaluation

What are some strengths and weaknesses of your model?

* * *

The strength of the model is that it can predict most of the houses’ price in Ames. But there are some certain houses, it overpredicted the price such as house #428,310,181. After removing the house #310, the accuracy of the model increases but when we look at the scale plot, tt can be seen that the variability (variances) of the residual points increases with the value of the fitted outcome variable, suggesting non-constant variances in the residuals errors. Thus, we should consider to use a log or square root transformation of the outcome variable (y) and some predictors besides area

* * *

### Section 4.4 Final Model Validation

Testing your final model on a separate, validation data set is a great way to determine how your model will perform in real-life practice. 

You will use the “ames_validation” dataset to do some additional assessment of your final model. Discuss your findings, be sure to mention:
* What is the RMSE of your final model when applied to the validation data?  
* How does this value compare to that of the training data and/or testing data?
* What percentage of the 95% predictive confidence (or credible) intervals contain the true price of the house in the validation data set?  
* From this result, does your final model properly reflect uncertainty?

```{r loadvalidation, message = FALSE}
load("ames_validation.Rdata")
```

* * *

We will use the ames_validation dataset to do some additional assessment of your final model especially to find the RMSE and compare to that of the training data and/or testing data and the coverage

```{r model_validate}
# RMSE
pred.v.HPM <- predict(model.final, ames_validation, 
                    estimator="HPM", 
                    prediction=TRUE, se.fit=TRUE)
```

``` {r}
v_rmse <- sqrt(mean((exp(pred.v.HPM$fit) - ames_validation$price)^2))
v_rmse
```

``` {r}
# Get dataset of predictions and confidence intervals
out = as.data.frame(cbind(exp(confint(pred.v.HPM)),
                          price = ames_validation$price))
```

``` {r}
# Fix names in dataset
colnames(out)[1:2] <- c("lwr", "upr")  #fix names
```

``` {r}
# Get Coverage
pred.v.HPM.coverage <- out %>% summarize(cover = sum(price >= lwr & price <= upr)/n())
pred.v.HPM.coverage
```

Using the credible intervals from the validation predictions, 97.38% of all the credible intervals contain the true price of the house in the validation set. Using the median probability model to generate out-of-sample predictions and a 97.38% prediction interval, the proportion of observations (rows) in ames_validation have sales prices that fall outside the prediction intervals is about 2% which is lower than 5% so the model handles well uncertainty.

* * *

## Part 5 Conclusion

Provide a brief summary of your results, and a brief discussion of what you have learned about the data and your model. 

* * *

We can predict the house price from using the BMA model under ames_train data. Next, using teting data and validation data to check the accuracy of the model is important when performing diagnostic test of a model. In general, the model is built on the training data so overfitting often occurs. However, there is some exceptions. The testing data using for our project tends to perform better in predicting the price.

In case that we have a lot of variables to choose as predictors, we should not only base on variables that show strong quantative information. Like in this model, corrplot showing more than 10 variables having strong relationship with price but there are some variables which actually affected the price of a house are excluded.

Also, the model overpredicted certain houses such as house #310,428 and 181 so we need to do deep analysis about these houses. After looking all information about house #310, we see that excluding this house from our analysis will not affect our model because the actual price reflects exactly the status of the house. Moreover, when building a model, we should always diagnotic the model about residuals vs fitted, scales plot, the Q-Q plot to see how our model behaves and adjust the model to have a better result.

In short, to have a good model we must obtain a good data, explore the data to understand well about data, build model, diagnotic the model, test the model and validate the model.

* * *
